{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸŒ NLLB-200 Translation Service â€” GPU T4\n",
    "\n",
    "Lance le service de traduction et se connecte automatiquement Ã  votre application.\n",
    "\n",
    "**Une seule Ã©tape :** `ExÃ©cution â†’ Tout exÃ©cuter`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# â”€â”€ Cellule 1 : Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# URL de votre application (ne pas modifier si vous utilisez le serveur par dÃ©faut)\n",
    "VM_URL = \"http://144.126.203.233:3001\"\n",
    "# Token de sÃ©curitÃ© (doit correspondre Ã  COLAB_SECRET dans le backend)\n",
    "COLAB_SECRET = \"docuLens2024\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# â”€â”€ Cellule 2 : Installation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "!pip install -q transformers sentencepiece torch fastapi uvicorn nest_asyncio\n",
    "!curl -sL https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -o /usr/local/bin/cloudflared\n",
    "!chmod +x /usr/local/bin/cloudflared\n",
    "print('âœ… DÃ©pendances installÃ©es')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# â”€â”€ Cellule 3 : Service FastAPI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "service_code = '''\n",
    "import os, logging, asyncio\n",
    "from contextlib import asynccontextmanager\n",
    "from typing import List\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_NAME  = os.getenv(\"NLLB_MODEL\", \"facebook/nllb-200-distilled-600M\")\n",
    "MAX_LENGTH  = int(os.getenv(\"NLLB_MAX_LENGTH\", \"512\"))\n",
    "BATCH_SIZE  = int(os.getenv(\"NLLB_BATCH_SIZE\", \"32\"))\n",
    "PORT        = int(os.getenv(\"TRANSLATION_PORT\", \"5001\"))\n",
    "\n",
    "SUPPORTED_LANGUAGES = {\n",
    "    \"fra_Latn\": \"FranÃ§ais\",\n",
    "    \"eng_Latn\": \"Anglais\",\n",
    "    \"arb_Arab\": \"Arabe (standard)\",\n",
    "    \"hau_Latn\": \"Haoussa\",\n",
    "    \"yor_Latn\": \"Yoruba\",\n",
    "    \"swh_Latn\": \"Swahili\",\n",
    "    \"zho_Hans\": \"Chinois simplifiÃ©\",\n",
    "    \"spa_Latn\": \"Espagnol\",\n",
    "    \"por_Latn\": \"Portugais\",\n",
    "    \"deu_Latn\": \"Allemand\",\n",
    "}\n",
    "\n",
    "state = {\"model\": None, \"tokenizer\": None, \"device\": \"cpu\", \"ready\": False}\n",
    "\n",
    "def load_model():\n",
    "    from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "    import torch\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    log.info(f\"Chargement {MODEL_NAME} sur {device.upper()}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    "    )\n",
    "    model.to(device).eval()\n",
    "    state.update({\"tokenizer\": tokenizer, \"model\": model, \"device\": device, \"ready\": True})\n",
    "    log.info(f\"Modele pret sur {device.upper()}\")\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    await loop.run_in_executor(None, load_model)\n",
    "    yield\n",
    "\n",
    "app = FastAPI(title=\"NLLB Translation Service\", lifespan=lifespan)\n",
    "app.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_methods=[\"*\"], allow_headers=[\"*\"])\n",
    "\n",
    "class TranslateRequest(BaseModel):\n",
    "    texts: List[str]\n",
    "    src_lang: str = \"fra_Latn\"\n",
    "    tgt_lang: str = \"eng_Latn\"\n",
    "\n",
    "class TranslateResponse(BaseModel):\n",
    "    translations: List[str]\n",
    "    src_lang: str\n",
    "    tgt_lang: str\n",
    "    count: int\n",
    "\n",
    "def translate_batch(texts, src_lang, tgt_lang):\n",
    "    import torch\n",
    "    tokenizer, model, device = state[\"tokenizer\"], state[\"model\"], state[\"device\"]\n",
    "    results = []\n",
    "    for i in range(0, len(texts), BATCH_SIZE):\n",
    "        batch = texts[i:i+BATCH_SIZE]\n",
    "        non_empty_idx  = [j for j, t in enumerate(batch) if t.strip()]\n",
    "        non_empty_text = [batch[j] for j in non_empty_idx]\n",
    "        batch_results  = [\"\"] * len(batch)\n",
    "        if non_empty_text:\n",
    "            tokenizer.src_lang = src_lang\n",
    "            inputs = tokenizer(\n",
    "                non_empty_text, return_tensors=\"pt\",\n",
    "                padding=True, truncation=True, max_length=MAX_LENGTH\n",
    "            ).to(device)\n",
    "            forced_bos = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
    "            with torch.no_grad():\n",
    "                output_ids = model.generate(\n",
    "                    **inputs, forced_bos_token_id=forced_bos,\n",
    "                    max_length=MAX_LENGTH, num_beams=4\n",
    "                )\n",
    "            translated = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "            for idx, t in zip(non_empty_idx, translated):\n",
    "                batch_results[idx] = t\n",
    "        results.extend(batch_results)\n",
    "    return results\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    return {\"status\": \"ready\" if state[\"ready\"] else \"loading\", \"model\": MODEL_NAME, \"device\": state[\"device\"]}\n",
    "\n",
    "@app.get(\"/languages\")\n",
    "def languages():\n",
    "    return {\"languages\": SUPPORTED_LANGUAGES}\n",
    "\n",
    "@app.post(\"/translate\", response_model=TranslateResponse)\n",
    "async def translate(req: TranslateRequest):\n",
    "    if not state[\"ready\"]:\n",
    "        raise HTTPException(503, detail=\"Modele en chargement\")\n",
    "    if req.src_lang == req.tgt_lang:\n",
    "        raise HTTPException(400, detail=\"src_lang et tgt_lang doivent etre differents\")\n",
    "    log.info(f\"Traduction {req.src_lang} -> {req.tgt_lang} | {len(req.texts)} textes\")\n",
    "    loop = asyncio.get_event_loop()\n",
    "    translations = await loop.run_in_executor(None, translate_batch, req.texts, req.src_lang, req.tgt_lang)\n",
    "    return TranslateResponse(\n",
    "        translations=translations, src_lang=req.src_lang,\n",
    "        tgt_lang=req.tgt_lang, count=len([t for t in translations if t])\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=PORT, log_level=\"info\")\n",
    "'''\n",
    "with open('/content/translation_service.py', 'w') as f:\n",
    "    f.write(service_code)\n",
    "print('âœ… Service Ã©crit')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# â”€â”€ Cellule 4 : Lancement uvicorn â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import subprocess, time, requests, nest_asyncio, threading\n",
    "nest_asyncio.apply()\n",
    "\n",
    "proc = subprocess.Popen(\n",
    "    ['uvicorn', 'translation_service:app', '--host', '0.0.0.0', '--port', '5001'],\n",
    "    cwd='/content', stdout=subprocess.PIPE, stderr=subprocess.STDOUT\n",
    ")\n",
    "threading.Thread(target=lambda: [print(l.decode('utf-8','replace'), end='') for l in proc.stdout], daemon=True).start()\n",
    "\n",
    "print('â³ Chargement du modÃ¨le sur GPU...')\n",
    "for i in range(90):\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        r = requests.get('http://localhost:5001/health', timeout=2)\n",
    "        if r.json().get('status') == 'ready':\n",
    "            print(f'\\nâœ… ModÃ¨le prÃªt sur {r.json()[\"device\"].upper()}')\n",
    "            break\n",
    "    except: pass\n",
    "else:\n",
    "    print('âŒ Timeout')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# â”€â”€ Cellule 5 : Tunnel Cloudflare + connexion automatique Ã  la VM â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import subprocess, re, requests, time\n",
    "\n",
    "tunnel = subprocess.Popen(\n",
    "    ['cloudflared', 'tunnel', '--url', 'http://localhost:5001'],\n",
    "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT\n",
    ")\n",
    "\n",
    "PUBLIC_URL = None\n",
    "print('â³ Ouverture du tunnel...')\n",
    "for line in tunnel.stdout:\n",
    "    text = line.decode('utf-8', errors='replace')\n",
    "    match = re.search(r'https://[\\w-]+\\.trycloudflare\\.com', text)\n",
    "    if match:\n",
    "        PUBLIC_URL = match.group(0)\n",
    "        break\n",
    "\n",
    "print(f'\\nğŸŒ URL : {PUBLIC_URL}')\n",
    "\n",
    "# Envoyer automatiquement l'URL Ã  la VM\n",
    "print('\\nğŸ“¡ Connexion automatique Ã  l\\'application...')\n",
    "connected = False\n",
    "for attempt in range(5):\n",
    "    try:\n",
    "        r = requests.post(\n",
    "            f'{VM_URL}/api/config/translation-url',\n",
    "            json={'url': PUBLIC_URL, 'secret': COLAB_SECRET},\n",
    "            timeout=10\n",
    "        )\n",
    "        if r.status_code == 200:\n",
    "            print('\\n' + '=' * 60)\n",
    "            print('ğŸš€ GPU Colab connectÃ© Ã  votre application !')\n",
    "            print('   Retournez sur l\\'app â†’ le badge passe au vert')\n",
    "            print('=' * 60)\n",
    "            connected = True\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f'   Tentative {attempt+1}/5... ({e})')\n",
    "        time.sleep(3)\n",
    "\n",
    "if not connected:\n",
    "    print('\\nâš ï¸  Connexion automatique Ã©chouÃ©e.')\n",
    "    print(f'   Copiez manuellement cette URL dans l\\'app : {PUBLIC_URL}')"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
