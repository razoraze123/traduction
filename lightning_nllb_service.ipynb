{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ğŸŒ NLLB-200 Translation Service â€” Lightning AI\n",
    "\n",
    "Lance le service de traduction NLLB-200 sur GPU Lightning AI et se connecte automatiquement Ã  votre application.\n",
    "\n",
    "**PrÃ©requis :**\n",
    "1. CrÃ©er un compte sur [lightning.ai](https://lightning.ai) (15 crÃ©dits GPU/mois offerts)\n",
    "2. CrÃ©er un nouveau **Studio** â†’ choisir un GPU (A10G recommandÃ©)\n",
    "3. Ouvrir ce notebook dans le Studio\n",
    "\n",
    "**Une seule Ã©tape :** `Run All`\n",
    "\n",
    "> **Avantage Lightning AI** : Le stockage est persistant â€” le modÃ¨le NLLB se tÃ©lÃ©charge une seule fois (~/50 Go disponibles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cellule 1 : Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# URL de votre application (modifier avec l'IP de votre VM)\n",
    "VM_URL = \"http://144.126.203.233:3001\"\n",
    "# Token de sÃ©curitÃ© (doit correspondre Ã  COLAB_SECRET dans le backend)\n",
    "COLAB_SECRET = \"docuLens2024\"\n",
    "# Port du service\n",
    "PORT = 5001\n",
    "# Chemin de stockage persistant (Lightning AI conserve /teamspace entre sessions)\n",
    "WORK_DIR = \"/teamspace/studios/this_studio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cellule 2 : Installation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import subprocess, sys, os\n",
    "\n",
    "print('ğŸ“¦ Installation des dÃ©pendances...')\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', '-q',\n",
    "    'transformers', 'sentencepiece', 'fastapi', 'uvicorn[standard]',\n",
    "    'nest_asyncio', 'accelerate'], check=True)\n",
    "\n",
    "# Cloudflare tunnel (pour exposer le service publiquement)\n",
    "if not os.path.exists('/usr/local/bin/cloudflared'):\n",
    "    subprocess.run([\n",
    "        'curl', '-sL',\n",
    "        'https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64',\n",
    "        '-o', '/usr/local/bin/cloudflared'\n",
    "    ], check=True)\n",
    "    subprocess.run(['chmod', '+x', '/usr/local/bin/cloudflared'], check=True)\n",
    "    print('âœ… cloudflared installÃ©')\n",
    "else:\n",
    "    print('âœ… cloudflared dÃ©jÃ  prÃ©sent')\n",
    "\n",
    "import torch\n",
    "gpu = torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU (pas de GPU dÃ©tectÃ©)'\n",
    "print(f'âœ… Installation terminÃ©e â€” GPU : {gpu}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cellule 3 : Service FastAPI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "\n",
    "service_code = '''\n",
    "import os, logging, asyncio\n",
    "from contextlib import asynccontextmanager\n",
    "from typing import List\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# Sur Lightning AI, le cache HuggingFace est persistant entre sessions\n",
    "MODEL_NAME = os.getenv(\"NLLB_MODEL\", \"facebook/nllb-200-distilled-600M\")\n",
    "MAX_LENGTH = int(os.getenv(\"NLLB_MAX_LENGTH\", \"512\"))\n",
    "BATCH_SIZE = int(os.getenv(\"NLLB_BATCH_SIZE\", \"32\"))\n",
    "PORT       = int(os.getenv(\"TRANSLATION_PORT\", \"5001\"))\n",
    "\n",
    "state = {\"model\": None, \"tokenizer\": None, \"device\": \"cpu\", \"ready\": False}\n",
    "\n",
    "def load_model():\n",
    "    from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "    import torch\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    log.info(f\"Chargement {MODEL_NAME} sur {device.upper()}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    "    )\n",
    "    model.to(device).eval()\n",
    "    state.update({\"tokenizer\": tokenizer, \"model\": model, \"device\": device, \"ready\": True})\n",
    "    log.info(f\"Modele pret sur {device.upper()}\")\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    await loop.run_in_executor(None, load_model)\n",
    "    yield\n",
    "\n",
    "app = FastAPI(title=\"NLLB Translation Service\", lifespan=lifespan)\n",
    "app.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_methods=[\"*\"], allow_headers=[\"*\"])\n",
    "\n",
    "class TranslateRequest(BaseModel):\n",
    "    texts: List[str]\n",
    "    src_lang: str = \"fra_Latn\"\n",
    "    tgt_lang: str = \"eng_Latn\"\n",
    "\n",
    "def translate_batch(texts, src_lang, tgt_lang):\n",
    "    import torch\n",
    "    tokenizer, model, device = state[\"tokenizer\"], state[\"model\"], state[\"device\"]\n",
    "    results = []\n",
    "    for i in range(0, len(texts), BATCH_SIZE):\n",
    "        batch = texts[i:i+BATCH_SIZE]\n",
    "        non_empty_idx  = [j for j, t in enumerate(batch) if t.strip()]\n",
    "        non_empty_text = [batch[j] for j in non_empty_idx]\n",
    "        batch_results  = [\"\"] * len(batch)\n",
    "        if non_empty_text:\n",
    "            tokenizer.src_lang = src_lang\n",
    "            inputs = tokenizer(\n",
    "                non_empty_text, return_tensors=\"pt\",\n",
    "                padding=True, truncation=True, max_length=MAX_LENGTH\n",
    "            ).to(device)\n",
    "            forced_bos = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
    "            with torch.no_grad():\n",
    "                output_ids = model.generate(\n",
    "                    **inputs, forced_bos_token_id=forced_bos,\n",
    "                    max_length=MAX_LENGTH, num_beams=4\n",
    "                )\n",
    "            translated = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "            for idx, t in zip(non_empty_idx, translated):\n",
    "                batch_results[idx] = t\n",
    "        results.extend(batch_results)\n",
    "    return results\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    return {\"status\": \"ready\" if state[\"ready\"] else \"loading\", \"model\": MODEL_NAME, \"device\": state[\"device\"]}\n",
    "\n",
    "@app.post(\"/translate\")\n",
    "async def translate(req: TranslateRequest):\n",
    "    if not state[\"ready\"]:\n",
    "        raise HTTPException(503, detail=\"Modele en chargement\")\n",
    "    if req.src_lang == req.tgt_lang:\n",
    "        raise HTTPException(400, detail=\"src_lang et tgt_lang doivent etre differents\")\n",
    "    loop = asyncio.get_event_loop()\n",
    "    translations = await loop.run_in_executor(None, translate_batch, req.texts, req.src_lang, req.tgt_lang)\n",
    "    return {\"translations\": translations, \"src_lang\": req.src_lang, \"tgt_lang\": req.tgt_lang, \"count\": len([t for t in translations if t])}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=PORT, log_level=\"info\")\n",
    "'''\n",
    "\n",
    "service_path = os.path.join(WORK_DIR, 'translation_service.py')\n",
    "with open(service_path, 'w') as f:\n",
    "    f.write(service_code)\n",
    "print(f'âœ… Service Ã©crit dans {service_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cellule 4 : Lancement uvicorn â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import subprocess, time, requests, threading, os\n",
    "\n",
    "env = os.environ.copy()\n",
    "env['TRANSLATION_PORT'] = str(PORT)\n",
    "\n",
    "proc = subprocess.Popen(\n",
    "    ['python', '-m', 'uvicorn', 'translation_service:app',\n",
    "     '--host', '0.0.0.0', '--port', str(PORT)],\n",
    "    cwd=WORK_DIR,\n",
    "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, env=env\n",
    ")\n",
    "threading.Thread(\n",
    "    target=lambda: [print(l.decode('utf-8','replace'), end='') for l in proc.stdout],\n",
    "    daemon=True\n",
    ").start()\n",
    "\n",
    "print('â³ Chargement du modÃ¨le NLLB-200...')\n",
    "print('   Lightning AI : modÃ¨le mis en cache â†’ dÃ©marrage rapide aprÃ¨s la 1Ã¨re fois')\n",
    "\n",
    "for i in range(120):\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        r = requests.get(f'http://localhost:{PORT}/health', timeout=2)\n",
    "        if r.json().get('status') == 'ready':\n",
    "            d = r.json()\n",
    "            print(f'\\nâœ… ModÃ¨le prÃªt sur {d[\"device\"].upper()} â€” {d[\"model\"]}')\n",
    "            break\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print('âŒ Timeout â€” vÃ©rifiez les logs ci-dessus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cellule 5 : Tunnel Cloudflare + connexion automatique Ã  la VM â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import subprocess, re, requests, time\n",
    "\n",
    "tunnel = subprocess.Popen(\n",
    "    ['cloudflared', 'tunnel', '--url', f'http://localhost:{PORT}'],\n",
    "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT\n",
    ")\n",
    "\n",
    "PUBLIC_URL = None\n",
    "print('â³ Ouverture du tunnel Cloudflare...')\n",
    "for line in tunnel.stdout:\n",
    "    text = line.decode('utf-8', errors='replace')\n",
    "    match = re.search(r'https://[\\w-]+\\.trycloudflare\\.com', text)\n",
    "    if match:\n",
    "        PUBLIC_URL = match.group(0)\n",
    "        break\n",
    "\n",
    "print(f'\\nğŸŒ URL publique : {PUBLIC_URL}')\n",
    "\n",
    "# Connexion automatique Ã  la VM\n",
    "print('\\nğŸ“¡ Connexion Ã  l\\'application...')\n",
    "connected = False\n",
    "for attempt in range(5):\n",
    "    try:\n",
    "        r = requests.post(\n",
    "            f'{VM_URL}/api/config/translation-url',\n",
    "            json={'url': PUBLIC_URL, 'secret': COLAB_SECRET},\n",
    "            timeout=10\n",
    "        )\n",
    "        if r.status_code == 200:\n",
    "            print('\\n' + '=' * 60)\n",
    "            print('ğŸš€ GPU Lightning AI connectÃ© Ã  votre application !')\n",
    "            print('   Le badge GPU passe au vert dans l\\'app')\n",
    "            print('=' * 60)\n",
    "            connected = True\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f'   Tentative {attempt+1}/5... ({e})')\n",
    "        time.sleep(3)\n",
    "\n",
    "if not connected:\n",
    "    print('\\nâš ï¸  Connexion automatique Ã©chouÃ©e.')\n",
    "    print(f'   â†’ Copiez cette URL manuellement dans l\\'app : {PUBLIC_URL}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cellule 6 : Garder la session active â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Lightning AI coupe les sessions sans activitÃ© aprÃ¨s ~4h\n",
    "# Cette cellule maintient le service en vie\n",
    "import time\n",
    "\n",
    "print('âœ… Service actif â€” Lightning AI GPU en cours d\\'utilisation')\n",
    "print(f'   URL : {PUBLIC_URL}')\n",
    "print('   ArrÃªt propre : Kernel â†’ Interrupt')\n",
    "print()\n",
    "\n",
    "start = time.time()\n",
    "while True:\n",
    "    elapsed = int(time.time() - start)\n",
    "    h, m = divmod(elapsed // 60, 60)\n",
    "    try:\n",
    "        r = requests.get(f'http://localhost:{PORT}/health', timeout=2)\n",
    "        status = r.json().get('status', '?')\n",
    "        device = r.json().get('device', '?')\n",
    "    except:\n",
    "        status = 'unreachable'\n",
    "        device = '?'\n",
    "    print(f'   â± {h:02d}h{m:02d}m | {device.upper()} | {status}', end='\\r')\n",
    "    time.sleep(30)"
   ]
  }
 ]
}
